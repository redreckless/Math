{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import SGD ,Adagrad\n",
    "from scipy.io import loadmat, savemat\n",
    "from keras.models import model_from_json\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "import csv\n",
    "import configparser\n",
    "import collections\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "from os import listdir\n",
    "import skimage.transform\n",
    "from skimage import color\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "import numpy\n",
    "from datetime import datetime\n",
    "from scipy.spatial.distance import cdist,pdist,squareform\n",
    "import theano.sandbox\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "\n",
    "def load_model(json_path):  # Function to load the model\n",
    "    model = model_from_json(open(json_path).read())\n",
    "    return model\n",
    "\n",
    "def load_weights(model, weight_path):  # Function to load the model weights\n",
    "    dict2 = loadmat(weight_path)\n",
    "    dict = conv_dict(dict2)\n",
    "    i = 0\n",
    "    for layer in model.layers:\n",
    "        weights = dict[str(i)]\n",
    "        layer.set_weights(weights)\n",
    "        i += 1\n",
    "    return model\n",
    "\n",
    "def conv_dict(dict2):\n",
    "    i = 0\n",
    "    dict = {}\n",
    "    for i in range(len(dict2)):\n",
    "        if str(i) in dict2:\n",
    "            if dict2[str(i)].shape == (0, 0):\n",
    "                dict[str(i)] = dict2[str(i)]\n",
    "            else:\n",
    "                weights = dict2[str(i)][0]\n",
    "                weights2 = []\n",
    "                for weight in weights:\n",
    "                    if weight.shape in [(1, x) for x in range(0, 5000)]:\n",
    "                        weights2.append(weight[0])\n",
    "                    else:\n",
    "                        weights2.append(weight)\n",
    "                dict[str(i)] = weights2\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Video\n",
    "\n",
    "def load_dataset_One_Video_Features(Test_Video_Path):\n",
    "\n",
    "    VideoPath =Test_Video_Path\n",
    "    f = open(VideoPath, \"r\")\n",
    "    words = f.read().split()\n",
    "    num_feat = int(len(words) / 4096)\n",
    "    # Number of features per video to be loaded. In our case num_feat=32, as we divide the video into 32 segments. Note that\n",
    "    # we have already computed C3D features for the whole video and divided the video features into 32 segments.\n",
    "    print(num_feat)\n",
    "    count = -1;\n",
    "    VideoFeatues = []\n",
    "    for feat in range(0, num_feat):\n",
    "        feat_row1 = np.float32(words[feat * 4096:feat * 4096 + 4096])\n",
    "        count = count + 1\n",
    "        if count == 0:\n",
    "            VideoFeatues = feat_row1\n",
    "        if count > 0:\n",
    "            VideoFeatues = np.vstack((VideoFeatues, feat_row1))\n",
    "    AllFeatures = VideoFeatues\n",
    "\n",
    "    return  AllFeatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting testing...\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting testing...\")\n",
    "\n",
    "\n",
    "AllTest_Video_Path = './Test2'\n",
    "# AllTest_Video_Path contains C3D features (txt file)  of each video. Each file contains 32 features, each of 4096 dimensions.\n",
    "Results_Path = './Eval_Res/'\n",
    "# Results_Path is the folder where you can save your results\n",
    "Model_dir='./'\n",
    "# Model_dir is the folder where we have placed our trained weights\n",
    "weights_path = Model_dir + 'weights_L1L2.mat'\n",
    "# weights_path is Trained model weights\n",
    "\n",
    "model_path = Model_dir + 'model.json'\n",
    "\n",
    "if not os.path.exists(Results_Path):\n",
    "       os.makedirs(Results_Path)\n",
    "\n",
    "All_Test_files= [i for i in listdir(AllTest_Video_Path) if i[0] != '.']\n",
    "All_Test_files.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['finput.txt']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "All_Test_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "Total Time took: 0:00:00.139573\n"
     ]
    }
   ],
   "source": [
    "model=load_model(model_path)\n",
    "load_weights(model, weights_path)\n",
    "nVideos=len(All_Test_files)\n",
    "time_before = datetime.now()\n",
    "result = []\n",
    "for iv in range(nVideos):\n",
    "\n",
    "    Test_Video_Path = os.path.join(AllTest_Video_Path, All_Test_files[iv])\n",
    "    inputs=load_dataset_One_Video_Features(Test_Video_Path) # 32 segments features for one testing video\n",
    "    predictions = model.predict_on_batch(inputs)   # Get anomaly prediction for each of 32 video segments.\n",
    "    aa=All_Test_files[iv]\n",
    "    aa=aa[0:-4]\n",
    "    A_predictions_path = Results_Path + aa + '.mat'  # Save array of 1*32, containing anomaly score for each segment. Please see Evaluate Anomaly Detector to compute  ROC.\n",
    "    print (\"Total Time took: \" + str(datetime.now() - time_before))\n",
    "    result.append(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('1Explosion008_x264_000866.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[6.0897606e-04],\n",
       "        [1.3030178e-03],\n",
       "        [7.8127280e-02],\n",
       "        [9.9751425e-01],\n",
       "        [9.4574887e-01],\n",
       "        [4.0951747e-01],\n",
       "        [2.8301280e-02],\n",
       "        [1.1295226e-03],\n",
       "        [2.2240597e-04],\n",
       "        [1.7030432e-05],\n",
       "        [1.5427113e-06],\n",
       "        [2.1062846e-07],\n",
       "        [9.7279120e-08],\n",
       "        [4.7137394e-08],\n",
       "        [1.5695929e-07],\n",
       "        [4.7346130e-06],\n",
       "        [6.4792024e-05],\n",
       "        [1.9794323e-04],\n",
       "        [1.1276546e-03],\n",
       "        [3.8371286e-03],\n",
       "        [1.6858036e-02],\n",
       "        [7.7868536e-02],\n",
       "        [6.7855924e-02],\n",
       "        [1.0964286e-01],\n",
       "        [1.6335015e-01],\n",
       "        [2.2282764e-01],\n",
       "        [1.5309086e-01],\n",
       "        [2.2730345e-01],\n",
       "        [4.7287855e-01],\n",
       "        [5.8449858e-01],\n",
       "        [7.4868846e-01],\n",
       "        [7.8337359e-01]]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
